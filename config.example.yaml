# output-test-dir: [arg]
## this controls the location of snakemake_unit_tests output test directories.
## for snakemake --generate-unit-tests, this defaults to '.tests' and all
## tests are reported to '.tests/unit'; mirroring that behavior, your tests
## will be emitted under 'arg/unit'. note that 'arg' need not exist already,
## but if it does exist, contents under 'arg/unit' are likely to be overwritten
## or deleted, so be cautious.
## note that if you specify --output-test-dir at the command line, it will
## *supercede* the setting in this file.
output-test-dir: .tests.cam

# snakefile: [arg]
## the name of (and path to) the snakefile used to successfully run the pipeline
## for which you want tests. this argument will look something like
## '/path/to/pipeline/workflow/Snakefile' or '../pipeline/workflow/Snakefile'.
## though the snakefile doesn't have to be in a 'workflow' subdirectory,
## certain internal logic assumes this structure as of pre-alpha.
## note that if you specify --snakefile at the command line, it will
## *supercede* the setting in this file.
snakefile: ../chip-variant-selection/workflow/Snakefile

# pipeline-top-dir: [arg]
## installation directory of pipeline for which you want tests. this is needed
## to deal with relative paths for directories and files you want added
## into the minimal test workspaces. if you do not specify this parameter,
## internal logic will set it to [{snakefile directory}/..], assuming
## the snakefile is in a one-deep subdirectory of the pipeline.
## note that if you specify --pipeline-top-dir at the command line, it will
## *supercede* the setting in this file.
pipeline-top-dir: ../chip-variant-selection

# pipeline-run-dir: [arg]
## execution directory of tested pipeline. expressed as a path relative
## to pipeline-top-dir. so, if you have a pipeline installed in
## /path/to/pipeline, and you ran the pipeline in /path/to/pipeline/rundir,
## this should be set to 'rundir'. if you run at top-level, set this to '.'
## note that if you specify --pipeline-run-dir at the command line, it will
## *supercede* the setting in this file.
pipeline-run-dir: .

# inst-dir: [arg]
## location on your system of the snakemake_unit_tests subdirectory 'inst'.
## this folder contains schematic pytest files that will be mildly modified
## and installed into your test workspaces, and which can be customized for
## different test behaviors if you prefer. if you're running snakemake_unit_tests
## from a cloned directory from GitLab, this can be simply 'inst'; otherwise
## if you've cloned snakemake_unit_tests but are running it elsewhere, use
## /path/to/snakemake_unit_tests/inst. eventually, once this is in conda,
## you should set this to $CONDA_PREFIX/share/snakemake_unit_tests/inst.
## note that if you specify --inst-dir at the command line, it will
## *supercede* the setting in this file.
inst-dir: inst

# snakemake-log: [arg]
## log file from successful run of your pipeline, off of which you
## are generating unit tests. this will probably be something like
## '../path/to/pipeline/my.log' or equivalent. this is most generally
## the literal output of `snakemake -nF > my.log`.
## note that the log file controls which rules are considered for
## generating tests. if you're having issues, or if you have two logs
## because a pipeline broke halfway through and you fixed it, or something
## like that, you can go into the log and add or remove rules as you
## see fit; however, that can be a somewhat error-prone process.
## note that if you specify --snakemake-log at the command line,
## it will *supercede* the setting in this file.
snakemake-log: ../chip-variant-selection/test.log

# added-files: [scalar or sequence]
## one or more files that you want added to test workspaces, so that
## the minimal test snakefile can have access to them. an example might
## be your manifest file; this is also how config files are propagated.
## the syntax is the *relative path of the file from the installation
## directory for your pipeline*: that is, from 'pipeline-top-dir'.
## note that if you specify additional --added-files at the command
## line, they will be *added* to this list; they will not replace this list.
## if you want that behavior, remove entries from this file.
added-files:
  - config/config.yaml
  - config/manifest.tsv

# added-directories: [scalar or sequence]
## one or more directories that you want added to test workspaces, so that
## the minimal test snakefile can have access to them. an example might
## be your config subdirectory, or workflows/envs, or workflows/scripts.
## the syntax is the *relative path of the directory from the installation
## directory for your pipeline*: that is, from 'pipeline-top-dir'.
## note that directory contents are copied recursively. you only need
## to use this option for things that are *not explicitly tracked as inputs
## or outputs of your rules*
## note that if you specify additional --added-directories at the command
## line, they will be *added* to this list; they will not replace this list.
## if you want that behavior, remove entries from this file.
added-directories:
  - schemas
  - workflow/envs
  - workflow/scripts

# include-rules: [scalar or sequence]
## one or more rule names that should be included in the test generation
## process. note that this set of rules is designed to coexist with the
## `exclude-rules` entries below: if either list is unspecified, the specified
## list controls behavior, but if both lists are specified, only rules present
## in the include list and absent from the include list will be reported out.
## note that if you specify additional --include-rules at the command
## line, they will be *added* to this list; they will not replace this list.
## if you want that behavior, remove entries from this file.
include-rules:
  - good_rulename # this rule will actually be included if present in the log and snakefile
  - tabix_vcf     # since this rule is also present in the exclude list, it will *not* be included
# exclude-rules: [scalar or sequence]
## one or more rule names that should be skipped in the test generation
## process. there are various reasons why rules might fail: perhaps the intermediates
## are absent for some reason (e.g. flagged as temp()), or perhaps the pipeline
## run is incomplete but you still want tests, or perhaps a download rule
## can't access a web resource that has been removed.
## note that if you specify additional --exclude-rules at the command
## line, they will be *added* to this list; they will not replace this list.
## if you want that behavior, remove entries from this file.
exclude-rules:
  - filter_ClinVar
  - convert_genes_to_regions_ACMG
  - tabix_vcf
  - download_ClinVar
  - download_refseq_gtf
  - download_GWAS_catalog
  - download_PharmGKB
  - download_html_table_ACMG
  - download_ilmnid_rsid_GDA
  - download_ilmnid_rsid_MEGA
  - download_ucsc_rsid_bigbed

# exclude-patterns: [scalar or sequence]
## optionally exclude files based on patterns of their paths/filenames
## from comparison in downstream pytest infrastructure. this information
## will be emitted to the tracking config.yaml in {output-test-dir}/unit.
## this is exclusion is useful, for example, for:
##  - certain types of outputs that contain dates/timestamps or that are
##    expected to be different between runs
##  - log or temporary directories that are expected to have output that
##    is either deliberately untracked or variable between runs
## note that these patterns are python re regular expressions and will
## be matched as such; the '.' is natively interpreted as any character
## except newline, so to match the actual character '.', escape it; so,
## for example, to match tsv files, use the expression `\.tsv$`. for
## paths, if possible, surround the pattern with '/' delimiters to improve
## fidelity of matching.
exclude-patterns:
  - "\\.tbi$"
  - "\\.html$"
  - "\\.log$"
  - "\\.bai$"
  - "\\.idx$"
  - "/log/"
  - "/logs/"
  - "/performance_benchmarks/"
  - "temp/"
  - "tmp/"

# comparators: [optional list of dicts]
## optionally flag a set of file extensions for comparison with special handlers
## in downstream pytest infrastructure. this is particularly useful for:
## - binary output formats (e.g. png, jpg, bam) that would fail when interpreted
##   through text loading in pytest/python
## - tables with floating point precision entries, where direct comparison may
##   need to be adjusted for precision errors
## each comparator takes:
## - type: the built-in handler with support in this infrastructure. accepted values are:
##         - "byte": direct byte-wise comparison with `cmp`
##         - "frame": comparison as pandas data_frames
##         - "plaintext": direct text comparison with adjustment for inline comments and dates
## - patterns: a scalar or sequence of regular expressions used to control which
##             files are handled with the comparator; see `exclude-patterns` for more information
## - args: for some comparators, additional settings that are passed along to the downstream
##         handler (e.g. arguments to read_table and assert_frame_equal in pandas). for comparators
##         that accept arguments, if you want to have different sets of arguments applied
##         to different file patterns, add additional dicts with the same type but different args
## comparators are completely optional; if none are specified, or if a file matches
## none of the specified patterns, the file will be probed to detect whether it looks
## like it contains a binary format; if so, it will be compared with `cmp`; otherwise,
## it will be treated as plaintext.
comparators:
  - type: "byte"
    patterns:
      - "\\.jpg$"
      - "\\.jpeg$"
      - "\\.png$"
      - "\\.bam$"
  - type: "frame"
    patterns:
      - "\\.logistic\\.hybrid"
    args:
      atol: 0.00000001
      rtol: 0.00001
      header: "infer"
      index_col: ~
      check_like: no
      sep: "\t"
